{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45415b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/vila2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-15 03:00:05,742] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/vila2/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu121\n",
      "CUDA version: 12.1\n",
      "Flash Attention version: 2.5.8\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import importlib.util\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "from pydantic import BaseModel\n",
    "from termcolor import colored\n",
    "import time\n",
    "import llava\n",
    "from llava import conversation as clib\n",
    "from llava.media import Image as LlavaImage #, Video\n",
    "from llava.model.configuration_llava import JsonSchemaResponseFormat, ResponseFormat\n",
    "from llava.conversation import auto_set_conversation_mode\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.model.builder import load_pretrained_model\n",
    "import torch\n",
    "from transformers import AutoModel, AutoProcessor, AutoModelForVision2Seq, AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from transformers import GenerationConfig\n",
    "import flash_attn\n",
    "from awq.quantize.pre_quant import run_awq, apply_awq\n",
    "from awq.quantize.quantizer import pseudo_quantize_model_weight, real_quantize_model_weight\n",
    "from awq.utils.lm_eval_adaptor import LMEvalAdaptor\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Flash Attention version: {flash_attn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b18a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 GB\n",
      "Max allocated: 0.0 GB\n",
      "Cached: 0.0\n"
     ]
    }
   ],
   "source": [
    "# GPU Utilisation Check\n",
    "print(\"Allocated:\", torch.cuda.memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Max allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Cached:\", torch.cuda.memory_reserved() / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f1d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, prompt, num_iterations=10, **kwargs):\n",
    "    \"\"\"\n",
    "    Simple script to benchmark on provided test image. \n",
    "    Stats / function returns:\n",
    "        last_response:  The last model output.\n",
    "        avg_latency:    Average latency across iterations.\n",
    "        latencies:      List of latencies per iteration.\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    last_response = None\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        start_time = time.time()\n",
    "        print(\"Memory before:\", torch.cuda.memory_allocated()/1e9, \"GB\")\n",
    "        last_response = model.generate_content(prompt, **kwargs)\n",
    "        print(\"Memory after:\", torch.cuda.memory_allocated()/1e9, \"GB\")\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        latencies.append(elapsed)\n",
    "        \n",
    "        # Combined per-iteration message\n",
    "        print(colored(f\"Iteration {i+1}/{num_iterations} took {elapsed:.3f} seconds\", \"green\"))\n",
    "\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    print(colored(f\"\\nLast Response: {last_response}\", \"cyan\", attrs=[\"bold\"]))\n",
    "    print(colored(f\"Average inference time over {num_iterations} runs: {avg_latency:.3f} seconds\", \n",
    "                  \"magenta\", attrs=[\"bold\"]))\n",
    "    return last_response, avg_latency, latencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d58caa",
   "metadata": {},
   "source": [
    "### Experiment 1: Vanilla VILA1.5-3b (Vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5681f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-15 03:00:51.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllava.conversation\u001b[0m:\u001b[36mauto_set_conversation_mode\u001b[0m:\u001b[36m190\u001b[0m - \u001b[1mSetting conversation mode to `vicuna_v1` based on model name/path `Efficient-Large-Model/VILA1.5-3b`.\u001b[0m\n",
      "Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 93943.57it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 5.896849632263184 GB\n",
      "Max allocated: 5.896849632263184 GB\n",
      "Cached: 6.091796875\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Efficient-Large-Model/VILA1.5-3b\"\n",
    "# model_path = \"Efficient-Large-Model/VILA1.5-3b-AWQ\"\n",
    "conv_mode = \"vicuna_v1\"\n",
    "\n",
    "# Set conversation modes\n",
    "clib.default_conversation = clib.conv_templates[conv_mode].copy()\n",
    "auto_set_conversation_mode(model_path)\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "\n",
    "# Load model components\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path = model_path,\n",
    "    model_name = model_name,\n",
    "    model_base = None\n",
    ")\n",
    "\n",
    "# Send model to Cuda\n",
    "model.to(\"cuda\").eval()\n",
    "\n",
    "# GPU Utilisation Check\n",
    "print(\"Allocated:\", torch.cuda.memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Max allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Cached:\", torch.cuda.memory_reserved() / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7f9d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 1/10 took 5.661 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 2/10 took 5.331 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 3/10 took 5.189 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 4/10 took 5.239 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 5/10 took 5.233 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 6/10 took 5.291 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 7/10 took 5.181 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 8/10 took 5.144 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 9/10 took 5.198 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 10/10 took 5.164 seconds\u001b[0m\n",
      "\u001b[1m\u001b[36m\n",
      "Last Response: The image is a collage of three distinct images, each with its own unique content. \n",
      "\n",
      "1. The first image is a vibrant representation of a hockey player in action, captured in mid-motion. The player, donned in a striking blue jersey, is seen skillfully maneuvering the puck on the ice. The background is a blur of colors, suggesting the speed and dynamism of the game.\n",
      "\n",
      "2. The second image is a detailed diagram of a drug test. The diagram, rendered in shades of green and yellow, provides a clear and concise visual representation of the drug test process. It includes various stages and components, such as the collection of urine samples, the analysis of the samples, and the reporting of the results.\n",
      "\n",
      "3. The third image is a question and answer format, with a question posed at the top and a corresponding answer at the bottom. The question is about the temperature in Antarctica, specifically asking about the temperature in 1979. The answer, however, is not visible in the image.\n",
      "\n",
      "Each image is distinct, yet they all share a common theme of scientific and medical content, suggesting a connection between the images. The collage as a whole seems to be a visual exploration of these themes, possibly for educational or informational purposes.\u001b[0m\n",
      "\u001b[1m\u001b[35mAverage inference time over 10 runs: 5.263 seconds\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The image is a collage of three distinct images, each with its own unique content. \\n\\n1. The first image is a vibrant representation of a hockey player in action, captured in mid-motion. The player, donned in a striking blue jersey, is seen skillfully maneuvering the puck on the ice. The background is a blur of colors, suggesting the speed and dynamism of the game.\\n\\n2. The second image is a detailed diagram of a drug test. The diagram, rendered in shades of green and yellow, provides a clear and concise visual representation of the drug test process. It includes various stages and components, such as the collection of urine samples, the analysis of the samples, and the reporting of the results.\\n\\n3. The third image is a question and answer format, with a question posed at the top and a corresponding answer at the bottom. The question is about the temperature in Antarctica, specifically asking about the temperature in 1979. The answer, however, is not visible in the image.\\n\\nEach image is distinct, yet they all share a common theme of scientific and medical content, suggesting a connection between the images. The collage as a whole seems to be a visual exploration of these themes, possibly for educational or informational purposes.',\n",
       " 5.263052487373352,\n",
       " [5.66145133972168,\n",
       "  5.331193208694458,\n",
       "  5.188562870025635,\n",
       "  5.238983631134033,\n",
       "  5.232658863067627,\n",
       "  5.290583610534668,\n",
       "  5.181287050247192,\n",
       "  5.1442437171936035,\n",
       "  5.197590351104736,\n",
       "  5.163970232009888])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare generation config (max length, max new tokens, num beams, repetition penalty, temperature, top_k, top_p drawn from VILA default settings)\n",
    "gen_config = GenerationConfig(\n",
    "    do_sample               =   False,\n",
    "    max_length              =   2048,\n",
    "    max_new_tokens          =   2048,\n",
    "    num_beams               =   1,\n",
    "    repetition_penalty      =   1.0,\n",
    "    temperature             =   1.0,\n",
    "    top_k                   =   50,\n",
    "    top_p                   =   1.0,\n",
    "    use_cache               =   False         \n",
    ")\n",
    "\n",
    "image_path = \"/workspace/VILA/demo_images/demo_img.png\"\n",
    "media = LlavaImage(image_path)\n",
    "text_prompt = \"\" \\\n",
    "\"Please describe the image in detail\" \\\n",
    "\"\"\n",
    "prompt = [media, text_prompt]\n",
    "\n",
    "benchmark_model(\n",
    "    model               =   model, \n",
    "    prompt              =   prompt,\n",
    "    num_iterations      =   10, \n",
    "    generation_config   =   gen_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b32090",
   "metadata": {},
   "source": [
    "#### Experiment 2: Vila1.5-3b (AWQ Quantised, 4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "570097ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "real weight quantization...: 100%|██████████| 32/32 [01:48<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 2.425227165222168 GB\n",
      "Max allocated: 6.06181001663208 GB\n",
      "Cached: 2.52734375\n"
     ]
    }
   ],
   "source": [
    "# Move model back to CPU\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "# Perform quantisation\n",
    "from awq.quantize.quantizer import real_quantize_model_weight\n",
    "\n",
    "q_config = {\n",
    "    \"zero_point\": True,   # AWQ will compute automatically\n",
    "    \"q_group_size\": 128,  # required group size for quantization\n",
    "}\n",
    "\n",
    "real_quantize_model_weight(\n",
    "    model=model,\n",
    "    w_bit=4,\n",
    "    q_config=q_config\n",
    ")\n",
    "\n",
    "# Move model back to cuda\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# Check resource utilisation\n",
    "print(\"Allocated:\", torch.cuda.memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Max allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Cached:\", torch.cuda.memory_reserved() / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c76cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before: 2.60406784 GB\n",
      "Memory after: 2.60406784 GB\n",
      "\u001b[32mIteration 1/10 took 4.477 seconds\u001b[0m\n",
      "Memory before: 2.60406784 GB\n",
      "Memory after: 2.60406784 GB\n",
      "\u001b[32mIteration 2/10 took 4.480 seconds\u001b[0m\n",
      "Memory before: 2.60406784 GB\n",
      "Memory after: 2.60406784 GB\n",
      "\u001b[32mIteration 3/10 took 4.423 seconds\u001b[0m\n",
      "Memory before: 2.60406784 GB\n",
      "Memory after: 2.60406784 GB\n",
      "\u001b[32mIteration 4/10 took 4.535 seconds\u001b[0m\n",
      "Memory before: 2.60406784 GB\n",
      "Memory after: 2.60406784 GB\n",
      "\u001b[32mIteration 5/10 took 4.358 seconds\u001b[0m\n",
      "Memory before: 2.60406784 GB\n",
      "Memory after: 2.60406784 GB\n",
      "\u001b[32mIteration 6/10 took 4.343 seconds\u001b[0m\n",
      "Memory before: 2.60406784 GB\n",
      "Memory after: 2.60406784 GB\n",
      "\u001b[32mIteration 7/10 took 4.299 seconds\u001b[0m\n",
      "Memory before: 2.60406784 GB\n",
      "Memory after: 2.60406784 GB\n",
      "\u001b[32mIteration 8/10 took 4.459 seconds\u001b[0m\n",
      "Memory before: 2.60406784 GB\n",
      "Memory after: 2.60406784 GB\n",
      "\u001b[32mIteration 9/10 took 4.490 seconds\u001b[0m\n",
      "Memory before: 2.60406784 GB\n",
      "Memory after: 2.60406784 GB\n",
      "\u001b[32mIteration 10/10 took 4.497 seconds\u001b[0m\n",
      "\u001b[1m\u001b[36m\n",
      "Last Response: The image is a collage of three distinct images, each with its own unique content. \n",
      "\n",
      "The first image is a vibrant photograph of a hockey player in mid-action, donned in a striking blue and yellow uniform. The player is captured in a dynamic pose, suggesting a moment of intense play. The background is a stark white, which contrasts with the player's colorful attire and brings the focus entirely on the athlete.\n",
      "\n",
      "The second image is a striking infographic. It features a circular diagram with a variety of colored sections, each representing different aspects of a topic. The diagram is neatly organized, with each section clearly labeled, making it easy to understand the information presented.\n",
      "\n",
      "The third image is a photograph of a map. The map is detailed, with various locations marked and labeled, indicating a geographical context. The map is set against a white background, which enhances the visibility of the various locations and their labels.\n",
      "\n",
      "Each image is distinct, yet they all share a common theme of visual storytelling. The collage as a whole seems to be a creative exploration of different visual storytelling techniques.\u001b[0m\n",
      "\u001b[1m\u001b[35mAverage inference time over 10 runs: 4.436 seconds\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"The image is a collage of three distinct images, each with its own unique content. \\n\\nThe first image is a vibrant photograph of a hockey player in mid-action, donned in a striking blue and yellow uniform. The player is captured in a dynamic pose, suggesting a moment of intense play. The background is a stark white, which contrasts with the player's colorful attire and brings the focus entirely on the athlete.\\n\\nThe second image is a striking infographic. It features a circular diagram with a variety of colored sections, each representing different aspects of a topic. The diagram is neatly organized, with each section clearly labeled, making it easy to understand the information presented.\\n\\nThe third image is a photograph of a map. The map is detailed, with various locations marked and labeled, indicating a geographical context. The map is set against a white background, which enhances the visibility of the various locations and their labels.\\n\\nEach image is distinct, yet they all share a common theme of visual storytelling. The collage as a whole seems to be a creative exploration of different visual storytelling techniques.\",\n",
       " 4.4359471797943115,\n",
       " [4.476616621017456,\n",
       "  4.479767799377441,\n",
       "  4.422546863555908,\n",
       "  4.5353498458862305,\n",
       "  4.358109951019287,\n",
       "  4.342999696731567,\n",
       "  4.298599481582642,\n",
       "  4.458501577377319,\n",
       "  4.4903647899627686,\n",
       "  4.496615171432495])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare generation config (max length, max new tokens, num beams, repetition penalty, temperature, top_k, top_p drawn from VILA default settings)\n",
    "gen_config = GenerationConfig(\n",
    "    do_sample               =   False,\n",
    "    max_length              =   2048,\n",
    "    max_new_tokens          =   2048,\n",
    "    num_beams               =   1,\n",
    "    repetition_penalty      =   1.0,\n",
    "    temperature             =   1.0,\n",
    "    top_k                   =   50,\n",
    "    top_p                   =   1.0,\n",
    "    use_cache               =   False         \n",
    ")\n",
    "\n",
    "image_path = \"/workspace/VILA/demo_images/demo_img.png\"\n",
    "media = LlavaImage(image_path)\n",
    "text_prompt = \"\" \\\n",
    "\"Please describe the image in detail\" \\\n",
    "\"\"\n",
    "prompt = [media, text_prompt]\n",
    "\n",
    "benchmark_model(\n",
    "    model               =   model, \n",
    "    prompt              =   prompt,\n",
    "    num_iterations      =   10, \n",
    "    generation_config   =   gen_config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vila2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
