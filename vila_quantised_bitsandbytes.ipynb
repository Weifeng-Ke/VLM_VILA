{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96969e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/vila/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-14 03:46:32,137] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/vila/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import importlib.util\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "from pydantic import BaseModel\n",
    "from termcolor import colored\n",
    "import time\n",
    "import llava\n",
    "from llava import conversation as clib\n",
    "from llava.media import Image as LlavaImage #, Video\n",
    "from llava.model.configuration_llava import JsonSchemaResponseFormat, ResponseFormat\n",
    "from llava.conversation import auto_set_conversation_mode\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.model.builder import load_pretrained_model\n",
    "import torch\n",
    "from transformers import AutoModel, AutoProcessor, AutoModelForVision2Seq, AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "# from PIL import Image \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4cfde3",
   "metadata": {},
   "source": [
    "Memory utilisation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085c09bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 GB\n",
      "Max allocated: 0.0 GB\n",
      "Cached: 0.0\n"
     ]
    }
   ],
   "source": [
    "# GPU Utilisation Check\n",
    "print(\"Allocated:\", torch.cuda.memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Max allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Cached:\", torch.cuda.memory_reserved() / 1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e9c1b",
   "metadata": {},
   "source": [
    "Basic benchmarking script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac5e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, prompt, num_iterations=10, **kwargs):\n",
    "    \"\"\"\n",
    "    Simple script to benchmark on provided test image. \n",
    "    Stats / function returns:\n",
    "        last_response:  The last model output.\n",
    "        avg_latency:    Average latency across iterations.\n",
    "        latencies:      List of latencies per iteration.\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    last_response = None\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        start_time = time.time()\n",
    "        print(\"Memory before:\", torch.cuda.memory_allocated()/1e9, \"GB\")\n",
    "        last_response = model.generate_content(prompt, **kwargs)\n",
    "        print(\"Memory after:\", torch.cuda.memory_allocated()/1e9, \"GB\")\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        latencies.append(elapsed)\n",
    "        \n",
    "        # Combined per-iteration message\n",
    "        print(colored(f\"Iteration {i+1}/{num_iterations} took {elapsed:.3f} seconds\", \"green\"))\n",
    "\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    print(colored(f\"\\nLast Response: {last_response}\", \"cyan\", attrs=[\"bold\"]))\n",
    "    print(colored(f\"Average inference time over {num_iterations} runs: {avg_latency:.3f} seconds\", \n",
    "                  \"magenta\", attrs=[\"bold\"]))\n",
    "    return last_response, avg_latency, latencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5810e898",
   "metadata": {},
   "source": [
    "### Experiment 1: Vanilla VILA1.5-3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf98c7b",
   "metadata": {},
   "source": [
    "#### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40da8b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-14 03:49:09.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllava.conversation\u001b[0m:\u001b[36mauto_set_conversation_mode\u001b[0m:\u001b[36m190\u001b[0m - \u001b[1mSetting conversation mode to `vicuna_v1` based on model name/path `Efficient-Large-Model/VILA1.5-3b`.\u001b[0m\n",
      "Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 118051.60it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.32s/it]\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 5.896849632263184 GB\n",
      "Max allocated: 5.896849632263184 GB\n",
      "Cached: 6.091796875\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Efficient-Large-Model/VILA1.5-3b\"\n",
    "# model_path = \"Efficient-Large-Model/VILA1.5-3b-AWQ\"\n",
    "conv_mode = \"vicuna_v1\"\n",
    "\n",
    "# Set conversation modes\n",
    "clib.default_conversation = clib.conv_templates[conv_mode].copy()\n",
    "auto_set_conversation_mode(model_path)\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "\n",
    "# Load model components\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path = model_path,\n",
    "    model_name = model_name,\n",
    "    model_base = None\n",
    ")\n",
    "\n",
    "# Send model to Cuda\n",
    "model.to(\"cuda\").eval()\n",
    "\n",
    "# GPU Utilisation Check\n",
    "print(\"Allocated:\", torch.cuda.memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Max allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Cached:\", torch.cuda.memory_reserved() / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16250d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 1/10 took 5.267 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 2/10 took 5.134 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 3/10 took 5.218 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 4/10 took 5.204 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 5/10 took 5.244 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 6/10 took 5.313 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 7/10 took 5.061 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 8/10 took 5.216 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 9/10 took 5.155 seconds\u001b[0m\n",
      "Memory before: 6.33169408 GB\n",
      "Memory after: 6.33169408 GB\n",
      "\u001b[32mIteration 10/10 took 5.171 seconds\u001b[0m\n",
      "\u001b[1m\u001b[36m\n",
      "Last Response: The image is a collage of three distinct images, each with its own unique content. \n",
      "\n",
      "1. The first image is a vibrant representation of a hockey player in action, captured in mid-motion. The player, donned in a striking blue jersey, is seen skillfully maneuvering the puck on the ice. The background is a blur of colors, suggesting the speed and dynamism of the game.\n",
      "\n",
      "2. The second image is a detailed diagram of a drug test. The diagram, rendered in shades of green and yellow, provides a clear and concise visual representation of the drug test process. It includes various stages and components, such as the collection of urine samples, the analysis of the samples, and the reporting of the results.\n",
      "\n",
      "3. The third image is a question and answer format, with a question posed at the top and a corresponding answer at the bottom. The question is about the temperature in Antarctica, specifically asking about the temperature in 1979. The answer, however, is not visible in the image.\n",
      "\n",
      "Each image is distinct, yet they all share a common theme of scientific and medical content, suggesting a connection between the images. The collage as a whole seems to be a visual exploration of these themes, possibly for educational or informational purposes.\u001b[0m\n",
      "\u001b[1m\u001b[35mAverage inference time over 10 runs: 5.198 seconds\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The image is a collage of three distinct images, each with its own unique content. \\n\\n1. The first image is a vibrant representation of a hockey player in action, captured in mid-motion. The player, donned in a striking blue jersey, is seen skillfully maneuvering the puck on the ice. The background is a blur of colors, suggesting the speed and dynamism of the game.\\n\\n2. The second image is a detailed diagram of a drug test. The diagram, rendered in shades of green and yellow, provides a clear and concise visual representation of the drug test process. It includes various stages and components, such as the collection of urine samples, the analysis of the samples, and the reporting of the results.\\n\\n3. The third image is a question and answer format, with a question posed at the top and a corresponding answer at the bottom. The question is about the temperature in Antarctica, specifically asking about the temperature in 1979. The answer, however, is not visible in the image.\\n\\nEach image is distinct, yet they all share a common theme of scientific and medical content, suggesting a connection between the images. The collage as a whole seems to be a visual exploration of these themes, possibly for educational or informational purposes.',\n",
       " 5.198351407051087,\n",
       " [5.267392873764038,\n",
       "  5.133617401123047,\n",
       "  5.218457937240601,\n",
       "  5.203777551651001,\n",
       "  5.244264364242554,\n",
       "  5.313192844390869,\n",
       "  5.061481714248657,\n",
       "  5.215940952301025,\n",
       "  5.15472936630249,\n",
       "  5.170659065246582])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare generation config (max length, max new tokens, num beams, repetition penalty, temperature, top_k, top_p drawn from VILA default settings)\n",
    "gen_config = GenerationConfig(\n",
    "    do_sample               =   False,\n",
    "    max_length              =   2048,\n",
    "    max_new_tokens          =   2048,\n",
    "    num_beams               =   1,\n",
    "    repetition_penalty      =   1.0,\n",
    "    temperature             =   1.0,\n",
    "    top_k                   =   50,\n",
    "    top_p                   =   1.0,\n",
    "    use_cache               =   False         \n",
    ")\n",
    "\n",
    "image_path = \"/workspace/VILA/demo_images/demo_img.png\"\n",
    "media = LlavaImage(image_path)\n",
    "text_prompt = \"\" \\\n",
    "\"Please describe the image in detail\" \\\n",
    "\"\"\n",
    "prompt = [media, text_prompt]\n",
    "\n",
    "benchmark_model(\n",
    "    model               =   model, \n",
    "    prompt              =   prompt,\n",
    "    num_iterations      =   10, \n",
    "    generation_config   =   gen_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f56912",
   "metadata": {},
   "source": [
    "#### Experiment 2: Vila1.5-3b (BitsandBytes Quantised, 4bit)\n",
    "**PLEASE RESTART THE PYTHON KERNEL BEFORE THIS** and rerun the benchmark function from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa286a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/vila/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-14 03:54:01,934] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/vila/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import importlib.util\n",
    "import json\n",
    "import os\n",
    "\n",
    "from termcolor import colored\n",
    "import time\n",
    "import llava\n",
    "from llava import conversation as clib\n",
    "from llava.media import Image as LlavaImage #, Video\n",
    "from llava.model.configuration_llava import JsonSchemaResponseFormat, ResponseFormat\n",
    "from llava.conversation import auto_set_conversation_mode\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.model.builder import load_pretrained_model\n",
    "import torch\n",
    "from transformers import AutoModel, AutoProcessor, AutoModelForVision2Seq, AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from transformers import GenerationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45060512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-14 03:54:59.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllava.conversation\u001b[0m:\u001b[36mauto_set_conversation_mode\u001b[0m:\u001b[36m190\u001b[0m - \u001b[1mSetting conversation mode to `vicuna_v1` based on model name/path `Efficient-Large-Model/VILA1.5-3b`.\u001b[0m\n",
      "Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 125533.75it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.80s/it]\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 2.5285568237304688 GB\n",
      "Max allocated: 2.5285568237304688 GB\n",
      "Cached: 2.625\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Efficient-Large-Model/VILA1.5-3b\"\n",
    "conv_mode = \"vicuna_v1\"\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Set conversation modes\n",
    "clib.default_conversation = clib.conv_templates[conv_mode].copy()\n",
    "auto_set_conversation_mode(model_path)\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_name=model_name,\n",
    "    model_base=None,\n",
    "    load_4bit=False,             # <-- must be False\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16    # optional, for model config\n",
    ")\n",
    "\n",
    "# Send model to Cuda\n",
    "model.to(\"cuda\").eval()\n",
    "\n",
    "# GPU Utilisation Check\n",
    "print(\"Allocated:\", torch.cuda.memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Max allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\")\n",
    "print(\"Cached:\", torch.cuda.memory_reserved() / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97b63e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before: 2.715017216 GB\n",
      "Memory after: 2.715017216 GB\n",
      "\u001b[32mIteration 1/10 took 12.471 seconds\u001b[0m\n",
      "Memory before: 2.715017216 GB\n",
      "Memory after: 2.715017216 GB\n",
      "\u001b[32mIteration 2/10 took 12.368 seconds\u001b[0m\n",
      "Memory before: 2.715017216 GB\n",
      "Memory after: 2.715017216 GB\n",
      "\u001b[32mIteration 3/10 took 12.343 seconds\u001b[0m\n",
      "Memory before: 2.715017216 GB\n",
      "Memory after: 2.715017216 GB\n",
      "\u001b[32mIteration 4/10 took 12.377 seconds\u001b[0m\n",
      "Memory before: 2.715017216 GB\n",
      "Memory after: 2.715017216 GB\n",
      "\u001b[32mIteration 5/10 took 12.856 seconds\u001b[0m\n",
      "Memory before: 2.715017216 GB\n",
      "Memory after: 2.715017216 GB\n",
      "\u001b[32mIteration 6/10 took 12.895 seconds\u001b[0m\n",
      "Memory before: 2.715017216 GB\n",
      "Memory after: 2.715017216 GB\n",
      "\u001b[32mIteration 7/10 took 12.273 seconds\u001b[0m\n",
      "Memory before: 2.715017216 GB\n",
      "Memory after: 2.715017216 GB\n",
      "\u001b[32mIteration 8/10 took 12.247 seconds\u001b[0m\n",
      "Memory before: 2.715017216 GB\n",
      "Memory after: 2.715017216 GB\n",
      "\u001b[32mIteration 9/10 took 12.224 seconds\u001b[0m\n",
      "Memory before: 2.715017216 GB\n",
      "Memory after: 2.715017216 GB\n",
      "\u001b[32mIteration 10/10 took 12.275 seconds\u001b[0m\n",
      "\u001b[1m\u001b[36m\n",
      "Last Response: The image is a collage of various images and text, all related to the topic of temperature. The collage is divided into three sections, each with its own set of images and text.\n",
      "\n",
      "In the top left section, there's a map of the world with a legend on the left side. The legend indicates different colors for various types of temperature. The map shows the temperature of the Earth's surface in degrees Celsius.\n",
      "\n",
      "Moving to the top right section, there's a pie chart with a legend on the left side. The legend shows different types of temperature represented by different colors. The pie chart shows the temperature of the Earth's surface in degrees Celsius.\n",
      "\n",
      "The bottom section features a bar chart with a legend on the left side. The legend shows different types of temperature represented by different colors. The bar chart shows the temperature of the Earth's surface in degrees Celsius.\n",
      "\n",
      "The text in the image provides additional information about temperature, specifically about the temperature of the Earth's surface. It mentions that the temperature of the Earth's surface is measured in degrees Celsius. It also mentions that the temperature of the Earth's surface is measured in degrees Celsius.\n",
      "\n",
      "Overall, the image provides a comprehensive overview of temperature, using different types of charts and graphs to represent the data.\u001b[0m\n",
      "\u001b[1m\u001b[35mAverage inference time over 10 runs: 12.433 seconds\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"The image is a collage of various images and text, all related to the topic of temperature. The collage is divided into three sections, each with its own set of images and text.\\n\\nIn the top left section, there's a map of the world with a legend on the left side. The legend indicates different colors for various types of temperature. The map shows the temperature of the Earth's surface in degrees Celsius.\\n\\nMoving to the top right section, there's a pie chart with a legend on the left side. The legend shows different types of temperature represented by different colors. The pie chart shows the temperature of the Earth's surface in degrees Celsius.\\n\\nThe bottom section features a bar chart with a legend on the left side. The legend shows different types of temperature represented by different colors. The bar chart shows the temperature of the Earth's surface in degrees Celsius.\\n\\nThe text in the image provides additional information about temperature, specifically about the temperature of the Earth's surface. It mentions that the temperature of the Earth's surface is measured in degrees Celsius. It also mentions that the temperature of the Earth's surface is measured in degrees Celsius.\\n\\nOverall, the image provides a comprehensive overview of temperature, using different types of charts and graphs to represent the data.\",\n",
       " 12.432804059982299,\n",
       " [12.471399784088135,\n",
       "  12.367670059204102,\n",
       "  12.343079328536987,\n",
       "  12.377022743225098,\n",
       "  12.855677127838135,\n",
       "  12.894506454467773,\n",
       "  12.272676467895508,\n",
       "  12.247198104858398,\n",
       "  12.224298477172852,\n",
       "  12.27451205253601])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare generation config (max length, max new tokens, num beams, repetition penalty, temperature, top_k, top_p drawn from VILA default settings)\n",
    "gen_config = GenerationConfig(\n",
    "    do_sample               =   False,\n",
    "    max_length              =   2048,\n",
    "    max_new_tokens          =   2048,\n",
    "    num_beams               =   1,\n",
    "    repetition_penalty      =   1.0,\n",
    "    temperature             =   1.0,\n",
    "    top_k                   =   50,\n",
    "    top_p                   =   1.0,\n",
    "    use_cache               =   False         \n",
    ")\n",
    "\n",
    "image_path = \"/workspace/VILA/demo_images/demo_img.png\"\n",
    "media = LlavaImage(image_path)\n",
    "text_prompt = \"\" \\\n",
    "\"Please describe the image in detail\" \\\n",
    "\"\"\n",
    "prompt = [media, text_prompt]\n",
    "\n",
    "benchmark_model(\n",
    "    model               =   model, \n",
    "    prompt              =   prompt,\n",
    "    num_iterations      =   10, \n",
    "    generation_config   =   gen_config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vila",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
